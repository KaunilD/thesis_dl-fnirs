{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import tables\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Trevor Jordan Grant.\n",
    "default4: spatial/verbal\n",
    "\"\"\"\n",
    "# Dictionary is labeled such that task labels in conditions files will have\n",
    "# more than one multilabeling schema.\n",
    "\n",
    "# To add more multilabeling schema - include them in the task Dictionary.\n",
    "# (The sub-dictionary where the task label is the key.)\n",
    "\n",
    "# default4 = [\"VerbalWM\", \"SpatialWM\", \"VisualPerceptual\", \"AuditoryPerceptual\"]\n",
    "# every label in default 4 has discrete values of 'off', 'low', 'high'\n",
    "\n",
    "# default3 = [\"WM\", \"VisualPerceptual\", \"AuditoryPerceptual\"]\n",
    "# every label in default 3 has discrete values of 'off', 'low', 'high'\n",
    "\n",
    "cog_load_label_dict = {\n",
    "# Mindfulness task labels.\n",
    "                       \"nb\": {\n",
    "                              \"default4\": [\"high\", \"off\", \"low\", \"off\"],\n",
    "                              \"default3\": [\"high\", \"low\", \"off\"],\n",
    "                             },\n",
    "                       \"anb\": {\n",
    "                               \"default4\": [\"high\", \"off\", \"off\", \"low\"],\n",
    "                               \"default3\": [\"high\", \"off\", \"low\"],\n",
    "                              },\n",
    "                       \"ewm\": {\n",
    "                               \"default4\": [\"low\", \"off\", \"high\", \"off\"],\n",
    "                               \"default3\": [\"low\", \"high\", \"off\"]\n",
    "                              },\n",
    "                        \"cr\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"off\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"off\", \"off\"],\n",
    "                              },\n",
    "                        \"rt\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                        \"es\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                       \"gng\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "            \"adaptive_words\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                   \"go_nogo\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                     \"nback\": {\n",
    "                               \"default4\": [\"high\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"high\", \"low\", \"off\"],\n",
    "                              },\n",
    "                    \"posner\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                 \"simple_rt\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "             \"visual_search\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"high\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"high\", \"off\"],\n",
    "                              },\n",
    "                      }\n",
    "\n",
    "\n",
    "def strings_to_vectors(string_labels, as_list=False):\n",
    "    \"\"\"Maps strings in dict to interger values.\n",
    "    Args:\n",
    "        string_labels(list): The string label value of load.\n",
    "        as_list(bool): False, if True, return list instead of np.array()\n",
    "    Returns:\n",
    "        labels as np.array()\n",
    "    \"\"\"\n",
    "\n",
    "    maps = {\n",
    "            \"off\": 0,\n",
    "            \"low\": 1,\n",
    "            \"high\": 2,\n",
    "           }\n",
    "\n",
    "    if as_list:\n",
    "        return [maps[label] for label in string_labels]\n",
    "    return np.array([maps[label] for label in string_labels])\n",
    "\n",
    "\n",
    "def return_label(task, label_type=\"default3\", as_strings=False):\n",
    "    \"\"\"Returns a label from the cog_load_label_dict.\n",
    "    Args:\n",
    "        task(str): The task label from the coditions file.\n",
    "        label_type(string): The label schema used for the model.\n",
    "        as_strings(bool): False, if True, return string (in list) values instead.\n",
    "    Returns:\n",
    "        labels(np.array): Under defaults labels will be returned as interger\n",
    "        values in a np.array().\n",
    "    \"\"\"\n",
    "    if as_strings:\n",
    "        return cog_load_label_dict[task][label_type]\n",
    "    return strings_to_vectors(cog_load_label_dict[task][label_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_52_5x11_mat = {\n",
    "           1:[0,1],  2:[0,2],  3:[0,3],  4:[0,4],  5:[0,5],  6:[0,6],  7:[0,7],  8:[0,8],  9:[0,9], 10:[0,10], \n",
    "11:[1,0], 12:[1,1], 13:[1,2], 14:[1,3], 15:[1,4], 16:[1,5], 17:[1,6], 18:[1,7], 19:[1,8], 20:[1,9], 21:[1,10], \n",
    "          22:[2,1], 23:[2,2], 24:[2,3], 25:[2,4], 26:[2,5], 27:[2,6], 28:[2,7], 29:[2,8], 30:[2,9], 31:[2,10], \n",
    "32:[3,0], 33:[3,1], 34:[3,2], 35:[3,3], 36:[3,4], 37:[3,5], 38:[3,6], 39:[3,7], 40:[3,8], 41:[3,9], 42:[3,10], \n",
    "          43:[4,1], 44:[4,2], 45:[4,3], 46:[4,4], 47:[4,5], 48:[4,6], 49:[4,7], 50:[4,8], 51:[4,9], 52:[4,10]\n",
    "}\n",
    "\n",
    "def get_52_5x11_mat(data):\n",
    "    # returns a matrix of size 5x11.\n",
    "    mat = np.zeros((5, 11))\n",
    "    for idx, i in enumerate((data)):\n",
    "        loc = channel_52_5x11_mat[idx+1]\n",
    "        mat[loc[0], loc[1]] = i\n",
    "    return mat\n",
    "\n",
    "channel_52_5x22_mat = {\n",
    "           1:[0,1],  2:[0,3],  3:[0,5],  4:[0,7],  5:[0,9],  6:[0,11],  7:[0,13],  8:[0,15],  9:[0,17], 10:[0,19], \n",
    "11:[1,0], 12:[1,2], 13:[1,4], 14:[1,6], 15:[1,8], 16:[1,10], 17:[1,12], 18:[1,14], 19:[1,16], 20:[1,18], 21:[1,20], \n",
    "          22:[2,1], 23:[2,3], 24:[2,5], 25:[2,7], 26:[2,9], 27:[2,11], 28:[2,13], 29:[2,15], 30:[2,17], 31:[2,19], \n",
    "32:[3,0], 33:[3,2], 34:[3,4], 35:[3,6], 36:[3,8], 37:[3,10], 38:[3,12], 39:[3,14], 40:[3,16], 41:[3,18], 42:[3,20], \n",
    "          43:[4,1], 44:[4,3], 45:[4,5], 46:[4,7], 47:[4,9], 48:[4,11], 49:[4,13], 50:[4,15], 51:[4,17], 52:[4,19]\n",
    "}\n",
    "\n",
    "def get_52_5x22_mat(data):\n",
    "    # returns a matrix of size 5x11.\n",
    "    mat = np.zeros((5, 22))\n",
    "    for idx, i in enumerate((data)):\n",
    "        loc = channel_52_5x22_mat[idx+1]\n",
    "        mat[loc[0], loc[1]] = i\n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_tasks(tasks, min_dur):\n",
    "    collapsed_tasks = []\n",
    "    collapsed_tasks.append(\n",
    "        tasks[0]\n",
    "    )\n",
    "    for i in range(1, len(tasks)):\n",
    "        t1 = collapsed_tasks[-1]\n",
    "        t2 = tasks[i]\n",
    "        if t1[\"class\"] == t2[\"class\"] and (t2[\"duration\"] < min_dur and t1[\"duration\"] < min_dur):\n",
    "            \n",
    "            t1[\"data\"] = np.concatenate((t1[\"data\"], t2[\"data\"]), axis=0)\n",
    "            t1[\"duration\"]+=t2[\"duration\"]\n",
    "            t1[\"end\"]=t1[\"onset\"]+t2[\"duration\"]\n",
    "            # merge and append\n",
    "        else:\n",
    "            # just append\n",
    "            collapsed_tasks.append(t2)\n",
    "    return collapsed_tasks\n",
    "            \n",
    "def read_tasks(condition, data):\n",
    "    # conditions, data = csv, mat files\n",
    "    # tuple containing (class, onset, duration, offset, oxy_data, dxy_data)\n",
    "    print(condition)\n",
    "    tasks = []\n",
    "    # read conditions, data\n",
    "    c_data = pd.read_csv(condition)\n",
    "    m_data = loadmat(data)\n",
    "    # get oxy, dxy data\n",
    "    oxyDaya = m_data['nirs_data'][0][0][0]\n",
    "    dxyData = m_data['nirs_data'][0][0][1]\n",
    "    # iterate through all the tasks here now.\n",
    "    for idx, key in enumerate(list(c_data.keys())):\n",
    "        start = 0\n",
    "        end = 0\n",
    "        class_ = None\n",
    "        if 'Task' in key or 'all_benchmarks_fNIRS' in key:\n",
    "            # get start and end index of the task\n",
    "            if 'Task' in key:\n",
    "                start = int(c_data[key][0])\n",
    "                duration = int(c_data[key][1])\n",
    "                class_ = c_data[key][2]\n",
    "            else:\n",
    "                start = int(c_data[key][2])\n",
    "                duration = int(c_data[key][3]) \n",
    "                class_ = c_data[key][4]\n",
    "            if class_ == \"adaptive_words\" or class_ == \"posner\":\n",
    "                continue\n",
    "                \n",
    "            end = start + duration\n",
    "            \n",
    "            # visualize heatmap: \n",
    "            # sns.heatmap(get_52_mat(oxyDaya[0]))\n",
    "\n",
    "            oxy_series = oxyDaya[start:end, :]\n",
    "            dxy_series = dxyData[start:end, :]\n",
    "\n",
    "            # a 100x5x22 list\n",
    "            oxy_dxy_series_mat = np.zeros((duration,2, 5, 11))\n",
    "\n",
    "            for ts, (oxy_slice, dxy_slice) in enumerate(zip(oxy_series, dxy_series)):\n",
    "                oxy_slice = get_52_5x11_mat(oxy_slice)\n",
    "                dxy_slice = get_52_5x11_mat(dxy_slice)\n",
    "                \n",
    "                #oxy_dxy_series_mat[ts] = np.hstack([oxy_slice, dxy_slice])\n",
    "                oxy_dxy_series_mat[ts] = np.array([oxy_slice, dxy_slice])\n",
    "            tasks.append(\n",
    "                {\n",
    "                    \"class\": class_,\n",
    "                    \"onset\": start,\n",
    "                    \"end\": end,\n",
    "                    \"duration\": duration,\n",
    "                    \"data\" : oxy_dxy_series_mat\n",
    "                }\n",
    "            )\n",
    "    return tasks \n",
    "\n",
    "def pad_tasks(tasks):\n",
    "    lengths = [len(t[\"data\"]) for t in tasks]\n",
    "    #max_len = max(lengths)\n",
    "    max_len = 3000\n",
    "    for t in tasks:\n",
    "        padded_task = np.zeros(np.concatenate( ([max_len], t[\"data\"].shape[1:]) ))\n",
    "        padded_task[:min(t[\"duration\"], max_len)] = t[\"data\"][:min(max_len, t[\"duration\"])]\n",
    "        t[\"data\"] = padded_task\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = sorted(glob.glob('../../../data/multilabel/mats/mindfulness/*.csv'))\n",
    "data = sorted(glob.glob('../../../data/multilabel/mats/mindfulness/*.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/multilabel/mats/mindfulness\\2001_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2001_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2002_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2002_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2003_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2003_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2004_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2004_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2006_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2006_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2011_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2011_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2012_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2012_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2013_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2013_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2014_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2014_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2015_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2015_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2017_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2017_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2019_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8201_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8201_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8203_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8203_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8204_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8204_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8205_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8205_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8206_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8206_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8208_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8208_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8209_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8209_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8210_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8210_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8211_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8211_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8212_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8212_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8213_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8213_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8214_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8214_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8215_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8215_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8216_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8216_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8217_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8217_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8218_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8218_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8219_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8219_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8221_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8221_fNIRS_conditions_s2.csv\n"
     ]
    }
   ],
   "source": [
    "task_data = []\n",
    "time_series_length = 10\n",
    "\"\"\"\n",
    "default3 labels\n",
    "[ \n",
    "    wm, \n",
    "    v, \n",
    "    a\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "for idx, (cond, dat) in enumerate(zip(conditions, data)):\n",
    "    participant_id = os.path.basename(dat)[0:4]\n",
    "    tasks = read_tasks(cond, dat)\n",
    "    for t in tasks:\n",
    "        task_data.append(t)\n",
    "        task_data[-1][\"id\"] = participant_id\n",
    "        task_data[-1][\"wl_label\"] = return_label(task_data[-1][\"class\"])\n",
    "    task_data = collapse_tasks(task_data, min_dur=time_series_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = pad_tasks(task_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET wm, vl, al (off, low, high) label counts and counts for each type of task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gng': {'cnt': 118, 'ts': 850}, 'go_nogo': {'cnt': 0, 'ts': 0}, 'es': {'cnt': 118, 'ts': 700}, 'nback': {'cnt': 0, 'ts': 0}, 'nb': {'cnt': 118, 'ts': 900}, 'ewm': {'cnt': 118, 'ts': 2800}, 'rt': {'cnt': 118, 'ts': 600}, 'cr': {'cnt': 706, 'ts': 250}, 'simple_rt': {'cnt': 0, 'ts': 0}, 'adaptive_words': {'cnt': 0, 'ts': 0}, 'posner': {'cnt': 0, 'ts': 0}, 'anb': {'cnt': 115, 'ts': 1050}, 'visual_search': {'cnt': 0, 'ts': 0}} {'al': {0: 1296, 1: 115, 2: 0}, 'vl': {0: 821, 1: 472, 2: 118}, 'wm': {0: 1060, 1: 118, 2: 233}}\n"
     ]
    }
   ],
   "source": [
    "labels_bin = {\"wm\":{0:0, 1:0, 2:0}, \"vl\":{0:0, 1:0, 2:0}, \"al\":{0:0, 1:0, 2:0}}\n",
    "task_cond_bin = {i:{\"ts\":0, \"cnt\":0} for i in cog_load_label_dict}\n",
    "for t in task_data:\n",
    "    if not (int(t[\"id\"])//1000 == 7):\n",
    "        label = return_label(t[\"class\"])\n",
    "\n",
    "        task_cond_bin[t[\"class\"]][\"cnt\"] += 1\n",
    "        task_cond_bin[t[\"class\"]][\"ts\"] = t[\"duration\"]\n",
    "\n",
    "        labels_bin[\"wm\"][label[0]]+=1\n",
    "        labels_bin[\"vl\"][label[1]]+=1\n",
    "        labels_bin[\"al\"][label[2]]+=1\n",
    "        \n",
    "print(task_cond_bin, labels_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_taskdata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in task_data:\n",
    "    if t[\"id\"] not in participant_taskdata:\n",
    "        participant_taskdata[t[\"id\"]] = []\n",
    "    participant_taskdata[t[\"id\"]].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013', '8215', '2015', '8218', '8219', '8217', '2001', '8203', '8221', '8205', '2006', '8208', '2004', '8213', '2011', '2002', '2014', '8206', '8216', '8204', '8211', '8212', '2012', '8214', '2019', '2017', '8201', '8209', '2003', '8210']\n"
     ]
    }
   ],
   "source": [
    "participant_ids = list(participant_taskdata.keys())\n",
    "print(participant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013', '8215', '2015', '8218', '8219', '8217', '2001', '8203', '8221', '8205', '2006', '8208', '2004', '8213', '2011', '2002', '2014', '8206', '8216', '8204', '8211', '8212', '2012', '8214']\n",
      "['2019', '2017', '8201', '8209', '2003', '8210']\n"
     ]
    }
   ],
   "source": [
    "train_ids = participant_ids[:int(0.8*len(participant_ids))]\n",
    "val_ids = participant_ids[int(0.8*len(participant_ids)):]\n",
    "print(train_ids)\n",
    "print(val_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bin train a-nback and n-back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 96\n"
     ]
    }
   ],
   "source": [
    "train_labeled_task_bin = {'anb':[], 'nb':[]}\n",
    "for participant_id in train_ids:\n",
    "    for participant_task in participant_taskdata[participant_id]:\n",
    "        if participant_task[\"class\"] in list(train_labeled_task_bin.keys()):\n",
    "            train_labeled_task_bin[participant_task[\"class\"]].append(participant_task)\n",
    "\n",
    "print(len(train_labeled_task_bin[\"anb\"]), len(train_labeled_task_bin[\"nb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 22\n"
     ]
    }
   ],
   "source": [
    "val_labeled_task_bin = {'anb':[], 'nb':[]}\n",
    "for participant_id in val_ids:\n",
    "    for participant_task in participant_taskdata[participant_id]:\n",
    "        if participant_task[\"class\"] in list(val_labeled_task_bin.keys()):\n",
    "            val_labeled_task_bin[participant_task[\"class\"]].append(participant_task)\n",
    "\n",
    "print( len(val_labeled_task_bin[\"anb\"]), len(val_labeled_task_bin[\"nb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = {0:[], 1:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "# matching pairs\n",
    "for i in train_labeled_task_bin:\n",
    "    \n",
    "    lab_tasks_idx = [j for j in range(len(train_labeled_task_bin[i]))]\n",
    "    lab_tasks_perm = lab_tasks_idx.copy()\n",
    "    \n",
    "    shuffle(lab_tasks_perm)\n",
    "    \n",
    "    while True:\n",
    "        if not np.any(lab_tasks_idx == lab_tasks_perm):\n",
    "            break\n",
    "\n",
    "    for a in lab_tasks_idx:\n",
    "        for b in lab_tasks_perm:\n",
    "            if a == b : continue\n",
    "            train_pairs[0].append(\n",
    "                (\n",
    "                    train_labeled_task_bin[i][a][\"data\"][50:250], \n",
    "                    train_labeled_task_bin[i][b][\"data\"][50:250], \n",
    "                    0\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different pairs\n",
    "labels = train_labeled_task_bin.keys()\n",
    "label_pairs = [(\"anb\", \"nb\")]\n",
    "\n",
    "for lab1, lab2 in label_pairs:\n",
    "    for task1 in train_labeled_task_bin[lab1]:\n",
    "        for task2 in train_labeled_task_bin[lab2]:\n",
    "            train_pairs[1].append((\n",
    "                task1[\"data\"][50:250], \n",
    "                task2[\"data\"][50:250],\n",
    "                1\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18050 9120\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pairs[0]), len(train_pairs[1]))\n",
    "\n",
    "shuffle(train_pairs[0])\n",
    "shuffle(train_pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    write all data to disk as is\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "data_list = []\n",
    "for idx, data in enumerate(task_data):\n",
    "    data_list.append(data)\n",
    "np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness\\\\data\", data_list)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness\\\\data_siamese_train\", train_pairs)\n",
    "\"\"\"\n",
    "# save matching\n",
    "for idx, data in enumerate(train_pairs[0]):\n",
    "    np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//siamese//alvl//train//0//\" + str(idx), data)\n",
    "\n",
    "# save different\n",
    "for idx, data in enumerate(train_pairs[1]):\n",
    "    np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//siamese//alvl//train//1//\" + str(idx), data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_examples = {\"anb\":[\"nb\"], \"nb\":[\"anb\"]}\n",
    "val_pairs = []\n",
    "for i in val_labeled_task_bin:\n",
    "    for task in val_labeled_task_bin[i]:\n",
    "        \n",
    "        t2 = val_label_examples[i][0]\n",
    "        \n",
    "        val_pairs.append({\n",
    "            \"t1\": task[\"data\"],\n",
    "            \"t3\": random.choice(val_labeled_task_bin[t2])[\"data\"][50:250],\n",
    "            \"t4\": random.choice(val_labeled_task_bin[i])[\"data\"][50:250],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//siamese//alvl//data_siamese_val\", val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### siamese pairs validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_pairs_val = {0:[], 1:[]}\n",
    "# matching pairs\n",
    "for i in val_labeled_task_bin:\n",
    "    \n",
    "    lab_tasks_idx = [j for j in range(len(val_labeled_task_bin[i]))]\n",
    "    lab_tasks_perm = lab_tasks_idx.copy()\n",
    "    \n",
    "    shuffle(lab_tasks_perm)\n",
    "    \n",
    "    while True:\n",
    "        if not np.any(lab_tasks_idx == lab_tasks_perm):\n",
    "            break\n",
    "\n",
    "    for a in lab_tasks_idx:\n",
    "        for b in lab_tasks_perm:\n",
    "            if a == b : continue\n",
    "            siamese_pairs_val[0].append(\n",
    "                (\n",
    "                    val_labeled_task_bin[i][a][\"data\"][50:250], \n",
    "                    val_labeled_task_bin[i][b][\"data\"][50:250], \n",
    "                    0\n",
    "                )\n",
    "            )\n",
    "\n",
    "# different pairs\n",
    "labels = val_labeled_task_bin.keys()\n",
    "label_pairs = [(\"anb\", \"nb\")]\n",
    "\n",
    "for lab1, lab2 in label_pairs:\n",
    "    for task1 in val_labeled_task_bin[lab1]:\n",
    "        for task2 in val_labeled_task_bin[lab2]:\n",
    "            siamese_pairs_val[1].append((\n",
    "                task1[\"data\"][50:250], \n",
    "                task2[\"data\"][50:250],\n",
    "                1\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842 440\n"
     ]
    }
   ],
   "source": [
    "print(len(siamese_pairs_val[0]), len(siamese_pairs_val[1]))\n",
    "\n",
    "shuffle(siamese_pairs_val[0])\n",
    "shuffle(siamese_pairs_val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matching\n",
    "for idx, data in enumerate(siamese_pairs_val[0][0:10000]):\n",
    "    np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//siamese//alvl//test//0//\" + str(idx), data)\n",
    "\n",
    "# save different\n",
    "for idx, data in enumerate(siamese_pairs_val[1][0:10000]):\n",
    "    np.save(\"C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//siamese//alvl//test//1//\" + str(idx), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "sns.set();\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_1 = 79\n",
    "cond_2 = 67\n",
    "\n",
    "\n",
    "\n",
    "oxy_1 = [i[0, 1, 1] for i in task_data[cond_1][\"data\"]]\n",
    "dxy_1 = [i[1, 1, 1] for i in task_data[cond_1][\"data\"]]\n",
    "\n",
    "oxy_2 = [i[0, 1, 1] for i in task_data[cond_2][\"data\"]]\n",
    "dxy_2 = [i[1, 1, 1] for i in task_data[cond_2][\"data\"]]\n",
    "\n",
    "print(len(oxy_1), len(oxy_2))\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    task_data[cond_1][\"class\"]+\"-oxy\": oxy_1,\n",
    "    task_data[cond_2][\"class\"]+\"-oxy\": oxy_2,\n",
    "\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(19, 9))\n",
    "ax = sns.lineplot(data=df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i[\"class\"] for i in task_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
