{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from random import sample, choice\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Trevor Jordan Grant.\n",
    "default4: spatial/verbal\n",
    "\"\"\"\n",
    "# Dictionary is labeled such that task labels in conditions files will have\n",
    "# more than one multilabeling schema.\n",
    "\n",
    "# To add more multilabeling schema - include them in the task Dictionary.\n",
    "# (The sub-dictionary where the task label is the key.)\n",
    "\n",
    "# default4 = [\"VerbalWM\", \"SpatialWM\", \"VisualPerceptual\", \"AuditoryPerceptual\"]\n",
    "# every label in default 4 has discrete values of 'off', 'low', 'high'\n",
    "\n",
    "# default3 = [\"WM\", \"VisualPerceptual\", \"AuditoryPerceptual\"]\n",
    "# every label in default 3 has discrete values of 'off', 'low', 'high'\n",
    "\n",
    "cog_load_label_dict = {\n",
    "# Mindfulness task labels.\n",
    "                       \"nb\": {\n",
    "                              \"default4\": [\"high\", \"off\", \"low\", \"off\"],\n",
    "                              \"default3\": [\"high\", \"low\", \"off\"],\n",
    "                             },\n",
    "                       \"anb\": {\n",
    "                               \"default4\": [\"high\", \"off\", \"off\", \"low\"],\n",
    "                               \"default3\": [\"high\", \"off\", \"low\"],\n",
    "                              },\n",
    "                       \"ewm\": {\n",
    "                               \"default4\": [\"low\", \"off\", \"high\", \"off\"],\n",
    "                               \"default3\": [\"low\", \"high\", \"off\"]\n",
    "                              },\n",
    "                        \"cr\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"off\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"off\", \"off\"],\n",
    "                              },\n",
    "                        \"rt\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                        \"es\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                       \"gng\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "            \"adaptive_words\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                   \"go_nogo\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                     \"nback\": {\n",
    "                               \"default4\": [\"high\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"high\", \"low\", \"off\"],\n",
    "                              },\n",
    "                    \"posner\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                 \"simple_rt\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "             \"visual_search\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"high\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"high\", \"off\"],\n",
    "                              },\n",
    "                      }\n",
    "\n",
    "\n",
    "def strings_to_vectors(string_labels, as_list=False):\n",
    "    \"\"\"Maps strings in dict to interger values.\n",
    "    Args:\n",
    "        string_labels(list): The string label value of load.\n",
    "        as_list(bool): False, if True, return list instead of np.array()\n",
    "    Returns:\n",
    "        labels as np.array()\n",
    "    \"\"\"\n",
    "\n",
    "    maps = {\n",
    "            \"off\": 0,\n",
    "            \"low\": 1,\n",
    "            \"high\": 2,\n",
    "           }\n",
    "\n",
    "    if as_list:\n",
    "        return [maps[label] for label in string_labels]\n",
    "    return np.array([maps[label] for label in string_labels])\n",
    "\n",
    "\n",
    "def return_label(task, label_type=\"default3\", as_strings=False):\n",
    "    \"\"\"Returns a label from the cog_load_label_dict.\n",
    "    Args:\n",
    "        task(str): The task label from the coditions file.\n",
    "        label_type(string): The label schema used for the model.\n",
    "        as_strings(bool): False, if True, return string (in list) values instead.\n",
    "    Returns:\n",
    "        labels(np.array): Under defaults labels will be returned as interger\n",
    "        values in a np.array().\n",
    "    \"\"\"\n",
    "    if as_strings:\n",
    "        return cog_load_label_dict[task][label_type]\n",
    "    return strings_to_vectors(cog_load_label_dict[task][label_type])\n",
    "\n",
    "channel_52_5x11_mat = {\n",
    "           1:[0,1],  2:[0,2],  3:[0,3],  4:[0,4],  5:[0,5],  6:[0,6],  7:[0,7],  8:[0,8],  9:[0,9], 10:[0,10], \n",
    "11:[1,0], 12:[1,1], 13:[1,2], 14:[1,3], 15:[1,4], 16:[1,5], 17:[1,6], 18:[1,7], 19:[1,8], 20:[1,9], 21:[1,10], \n",
    "          22:[2,1], 23:[2,2], 24:[2,3], 25:[2,4], 26:[2,5], 27:[2,6], 28:[2,7], 29:[2,8], 30:[2,9], 31:[2,10], \n",
    "32:[3,0], 33:[3,1], 34:[3,2], 35:[3,3], 36:[3,4], 37:[3,5], 38:[3,6], 39:[3,7], 40:[3,8], 41:[3,9], 42:[3,10], \n",
    "          43:[4,1], 44:[4,2], 45:[4,3], 46:[4,4], 47:[4,5], 48:[4,6], 49:[4,7], 50:[4,8], 51:[4,9], 52:[4,10]\n",
    "}\n",
    "\n",
    "def get_52_5x11_mat(data):\n",
    "    # returns a matrix of size 5x11.\n",
    "    mat = np.zeros((5, 11), dtype=np.float64)\n",
    "    for idx, i in enumerate((data)):\n",
    "        loc = channel_52_5x11_mat[idx+1]\n",
    "        mat[loc[0], loc[1]] = i\n",
    "    return mat\n",
    "\n",
    "channel_52_5x22_mat = {\n",
    "           1:[0,1],  2:[0,3],  3:[0,5],  4:[0,7],  5:[0,9],  6:[0,11],  7:[0,13],  8:[0,15],  9:[0,17], 10:[0,19], \n",
    "11:[1,0], 12:[1,2], 13:[1,4], 14:[1,6], 15:[1,8], 16:[1,10], 17:[1,12], 18:[1,14], 19:[1,16], 20:[1,18], 21:[1,20], \n",
    "          22:[2,1], 23:[2,3], 24:[2,5], 25:[2,7], 26:[2,9], 27:[2,11], 28:[2,13], 29:[2,15], 30:[2,17], 31:[2,19], \n",
    "32:[3,0], 33:[3,2], 34:[3,4], 35:[3,6], 36:[3,8], 37:[3,10], 38:[3,12], 39:[3,14], 40:[3,16], 41:[3,18], 42:[3,20], \n",
    "          43:[4,1], 44:[4,3], 45:[4,5], 46:[4,7], 47:[4,9], 48:[4,11], 49:[4,13], 50:[4,15], 51:[4,17], 52:[4,19]\n",
    "}\n",
    "\n",
    "def get_52_5x22_mat(data):\n",
    "    # returns a matrix of size 5x11.\n",
    "    mat = np.zeros((5, 22), dtype=np.float64)+0.001\n",
    "    for idx, i in enumerate((data)):\n",
    "        loc = channel_52_5x22_mat[idx+1]\n",
    "        mat[loc[0], loc[1]] = i\n",
    "    return mat\n",
    "\n",
    "def collapse_tasks(tasks, min_dur):\n",
    "    collapsed_tasks = []\n",
    "    collapsed_tasks.append(\n",
    "        tasks[0]\n",
    "    )\n",
    "    for i in range(1, len(tasks)):\n",
    "        t1 = collapsed_tasks[-1]\n",
    "        t2 = tasks[i]\n",
    "        if t1[\"class\"] == t2[\"class\"] and (t2[\"duration\"] < min_dur and t1[\"duration\"] < min_dur):\n",
    "            \n",
    "            t1[\"data\"] = np.concatenate((t1[\"data\"], t2[\"data\"]), axis=0)\n",
    "            t1[\"duration\"]+=t2[\"duration\"]\n",
    "            t1[\"end\"]=t1[\"onset\"]+t2[\"duration\"]\n",
    "            # merge and append\n",
    "        else:\n",
    "            # just append\n",
    "            collapsed_tasks.append(t2)\n",
    "    return collapsed_tasks\n",
    "            \n",
    "def read_tasks(condition, data):\n",
    "    # conditions, data = csv, mat files\n",
    "    # tuple containing (class, onset, duration, offset, oxy_data, dxy_data)\n",
    "    print(condition)\n",
    "    tasks = []\n",
    "    # read conditions, data\n",
    "    c_data = pd.read_csv(condition)\n",
    "    m_data = loadmat(data)\n",
    "    # get oxy, dxy data\n",
    "    oxyDaya = m_data['nirs_data'][0][0][0]\n",
    "    dxyData = m_data['nirs_data'][0][0][1]\n",
    "    # iterate through all the tasks here now.\n",
    "    for idx, key in enumerate(list(c_data.keys())):\n",
    "        start = 0\n",
    "        end = 0\n",
    "        class_ = None\n",
    "        if 'Task' in key or 'all_benchmarks_fNIRS' in key:\n",
    "            # get start and end index of the task\n",
    "            if 'Task' in key:\n",
    "                start = int(c_data[key][0])\n",
    "                duration = int(c_data[key][1])\n",
    "                class_ = c_data[key][2]\n",
    "            else:\n",
    "                start = int(c_data[key][2])\n",
    "                duration = int(c_data[key][3]) \n",
    "                class_ = c_data[key][4]\n",
    "            if class_ == \"adaptive_words\" or class_ == \"posner\"or class_ == \"es\" :\n",
    "                continue\n",
    "                \n",
    "            end = start + duration\n",
    "            \n",
    "            # visualize heatmap: \n",
    "            # sns.heatmap(get_52_mat(oxyDaya[0]))\n",
    "\n",
    "            oxy_series = oxyDaya[start:end, :]\n",
    "            dxy_series = dxyData[start:end, :]\n",
    "            # a 100x5x22 list\n",
    "            oxy_dxy_series_mat = np.zeros((duration,1, 5, 11))\n",
    "\n",
    "            for ts, (oxy_slice, dxy_slice) in enumerate(zip(oxy_series, dxy_series)):\n",
    "                oxy_slice = get_52_5x11_mat(oxy_slice)\n",
    "                dxy_slice = get_52_5x11_mat(dxy_slice)\n",
    "                \n",
    "                #oxy_dxy_series_mat[ts] = np.hstack([oxy_slice, dxy_slice])\n",
    "                oxy_dxy_series_mat[ts] = oxy_slice\n",
    "            tasks.append(\n",
    "                {\n",
    "                    \"class\": class_,\n",
    "                    \"onset\": start,\n",
    "                    \"end\": end,\n",
    "                    \"duration\": duration,\n",
    "                    \"data\" : oxy_dxy_series_mat\n",
    "                }\n",
    "            )\n",
    "    return tasks \n",
    "\n",
    "def pad_tasks(tasks):\n",
    "    lengths = [len(t[\"data\"]) for t in tasks]\n",
    "    #max_len = max(lengths)\n",
    "    max_len = 3000\n",
    "    for t in tasks:\n",
    "        padded_task = np.zeros(np.concatenate( ([max_len], t[\"data\"].shape[1:]) ))\n",
    "        padded_task[:min(t[\"duration\"], max_len)] = t[\"data\"][:min(max_len, t[\"duration\"])]\n",
    "        t[\"data\"] = padded_task\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = sorted(glob.glob('../../../data/multilabel/mats/mindfulness/*.csv'))\n",
    "data = sorted(glob.glob('../../../data/multilabel/mats/mindfulness/*.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/multilabel/mats/mindfulness\\2001_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2001_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2002_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2002_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2003_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2003_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2004_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2004_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2006_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2006_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2011_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2011_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2012_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2012_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2013_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2013_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2014_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2014_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2015_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2015_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2017_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2017_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\2019_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8201_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8201_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8203_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8203_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8204_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8204_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8205_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8205_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8206_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8206_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8208_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8208_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8209_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8209_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8210_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8210_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8211_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8211_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8212_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8212_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8213_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8213_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8214_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8214_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8215_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8215_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8216_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8216_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8217_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8217_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8218_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8218_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8219_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8219_fNIRS_conditions_s2.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8221_fNIRS_conditions_s1.csv\n",
      "../../../data/multilabel/mats/mindfulness\\8221_fNIRS_conditions_s2.csv\n"
     ]
    }
   ],
   "source": [
    "task_data = []\n",
    "time_series_length = 10\n",
    "\"\"\"\n",
    "default3 labels\n",
    "[ \n",
    "    wm, \n",
    "    v, \n",
    "    a\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "for idx, (cond, dat) in enumerate(zip(conditions, data)):\n",
    "    \n",
    "    participant_id = os.path.basename(cond)[0:4]\n",
    "    \n",
    "    session_id = os.path.basename(cond) # etc.csv\n",
    "    session_id = session_id.split(\".\")[0] # etc\n",
    "    session_id = session_id[-2:]\n",
    "    \n",
    "    tasks = read_tasks(cond, dat)\n",
    "    for t in tasks:\n",
    "        task_data.append(t)\n",
    "        task_data[-1][\"participant_id\"] = participant_id\n",
    "        task_data[-1][\"session_id\"] = session_id\n",
    "        task_data[-1][\"wl_label\"] = return_label(task_data[-1][\"class\"])\n",
    "    task_data = collapse_tasks(task_data, min_dur=time_series_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8201', '2001', '2004', '8214', '2003', '8217', '8215', '2017', '8218', '8205', '8219', '8221', '8203', '8213']\n",
      "11\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "participant_taskdata = {}\n",
    "\n",
    "\n",
    "\n",
    "invalids = [\n",
    "'8210_s2','2003_s1','8208_s2','8203_s1','8204_s1','8212_s1','2014_s2','2006_s2','8208_s2','8206_s2','8203_s1','8213_s1','8204_s1','2004_s1','8201_s1','8215_s1','8219_s1','8211_s1','2001_s1','2006_s1','2006_s1','2006_s2','8210_s1','8210_s2','8210_s2','8218_s1','2017_s1','2019_s1','2003_s1','8209_s1','8208_s1','8208_s1','8208_s2','2012_s1','8220_s1','2007_s1','8205_s2','2011_s1','2011_s2','2015_s2','8204_s2','2013_s1','2013_s1','2013_s2','8212_s2','2002_s2','2004_s1','2004_s1','2014_s1','2014_s1','2014_s2','8215_s1','2001_s1','2006_s1','2006_s2','8210_s2','2017_s1','2019_s1','8209_s2','8208_s2','8208_s2','2012_s1','2012_s2','8216_s2','8216_s2','8206_s2','8206_s2','8220_s1','8220_s1','2007_s1','2007_s1','8203_s1','8203_s1','2011_s1','8213_s1','8213_s1','2015_s1','2015_s1','2015_s2','8204_s1','8204_s2','2013_s1','2013_s2','8212_s1','2002_s1','2002_s2','2004_s1','2004_s1','2014_s1','8201_s1','8201_s1','8219_s1','8219_s1','8211_s1','8211_s2','2001_s1','2001_s1','8221_s2','2006_s1','8210_s1','8210_s1','8210_s2','8218_s1','2017_s1','2019_s1','2019_s1','2003_s1','8209_s1','8209_s1','8209_s2','8208_s1','8208_s1','8208_s2','8208_s2','2012_s1','2012_s1','2012_s2','2012_s2','8216_s1','8216_s1','8216_s2','8216_s2','8206_s1','8220_s1','2007_s1','2007_s1','8205_s2','2015_s1','2013_s2','8219_s1','2006_s2','8210_s2'\n",
    "]\n",
    "\n",
    "\n",
    "for t in task_data:\n",
    "    if t[\"participant_id\"]+'_'+t[\"session_id\"] not in invalids:\n",
    "        if t[\"participant_id\"] not in participant_taskdata:\n",
    "            participant_taskdata[t[\"participant_id\"]] = []\n",
    "        participant_taskdata[t[\"participant_id\"]].append(t)\n",
    "\n",
    "\n",
    "\n",
    "participant_ids = list(participant_taskdata.keys())\n",
    "print(participant_ids)\n",
    "\n",
    "TIME_CROP_LENGTH = 300\n",
    "\n",
    "train_ids = participant_ids[:int(0.8*len(participant_ids))]\n",
    "val_ids = participant_ids[int(0.8*len(participant_ids)):]\n",
    "print(len(train_ids))\n",
    "print(len(val_ids))\n",
    "\n",
    "# #### Get total rows in wl = wm for each off, low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant_id in participant_taskdata:\n",
    "    durations = []\n",
    "    data = []\n",
    "    \n",
    "    for task in participant_taskdata[participant_id]:\n",
    "        data.append(task[\"data\"])\n",
    "        durations.append(task[\"duration\"])\n",
    "    \n",
    "    cat_tasks = np.concatenate(data)\n",
    "    mask = cat_tasks == 0\n",
    "    cat_tasks+=mask*0.0001\n",
    "    \n",
    "    cat_tasks_mean = np.mean(cat_tasks, axis=0)\n",
    "    cat_tasks_std = np.std(cat_tasks, axis=0)\n",
    "    cat_tasks-=cat_tasks_mean\n",
    "    cat_tasks/=cat_tasks_std\n",
    "    \n",
    "    current_ts = 0\n",
    "    for idx, task in enumerate(participant_taskdata[participant_id]):\n",
    "        task[\"data\"] = cat_tasks[current_ts:current_ts+durations[idx]]\n",
    "        \n",
    "        current_ts+=durations[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180, 26]\n"
     ]
    }
   ],
   "source": [
    "train_labeled_task_bin = {0:[], 1:[], 2:[]}\n",
    "for participant_id in train_ids:\n",
    "    for t in participant_taskdata[participant_id]:\n",
    "\n",
    "        wm_label = t[\"wl_label\"][1]\n",
    "        if wm_label in [0, 2]:\n",
    "            train_labeled_task_bin[wm_label].append(t[\"data\"][:TIME_CROP_LENGTH])\n",
    "print( [len(train_labeled_task_bin[0]), len(train_labeled_task_bin[2])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bin tasks into 0, 1, 2 workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gng': {'ts': 850, 'cnt': 118}, 'visual_search': {'ts': 0, 'cnt': 0}, 'nb': {'ts': 900, 'cnt': 118}, 'rt': {'ts': 600, 'cnt': 118}, 'simple_rt': {'ts': 0, 'cnt': 0}, 'ewm': {'ts': 2800, 'cnt': 118}, 'nback': {'ts': 0, 'cnt': 0}, 'es': {'ts': 0, 'cnt': 0}, 'adaptive_words': {'ts': 0, 'cnt': 0}, 'go_nogo': {'ts': 0, 'cnt': 0}, 'posner': {'ts': 0, 'cnt': 0}, 'anb': {'ts': 1050, 'cnt': 115}, 'cr': {'ts': 250, 'cnt': 706}}\n"
     ]
    }
   ],
   "source": [
    "labels_bin = {\"wm\":{0:[], 1:[], 2:[]}, \"vl\":{0:[], 1:[], 2:[]}, \"al\":{0:[], 1:[], 2:[]}}\n",
    "task_cond_bin = {i:{\"ts\":0, \"cnt\":0} for i in cog_load_label_dict}\n",
    "for t in task_data:\n",
    "    label = return_label(t[\"class\"])\n",
    "\n",
    "    task_cond_bin[t[\"class\"]][\"cnt\"] += 1\n",
    "    task_cond_bin[t[\"class\"]][\"ts\"] = t[\"duration\"]\n",
    "    labels_bin[\"wm\"][label[0]].append(t)\n",
    "    labels_bin[\"vl\"][label[1]].append(t)\n",
    "    labels_bin[\"al\"][label[2]].append(t)\n",
    "\n",
    "print(task_cond_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_largest_index_argpartition_v1(a, k):\n",
    "    idx = np.argpartition(-a.ravel(),k)[:k]\n",
    "    return np.column_stack(np.unravel_index(idx, a.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\r"
     ]
    }
   ],
   "source": [
    "wm_1 = choice(labels_bin[\"wm\"][0])\n",
    "wm_3 = choice(labels_bin[\"wm\"][2])\n",
    "for tdx in range(0,3000):\n",
    "    print(tdx, end='\\r')\n",
    "    left, center, right = wm_1[\"data\"][tdx], wm_2[\"data\"][tdx], wm_3[\"data\"][tdx]\n",
    "    l_mask, c_mask, r_mask = np.zeros(left[0].shape), np.zeros(center[0].shape), np.zeros(right[0].shape)\n",
    "    k = 5\n",
    "    l_top_k = k_largest_index_argpartition_v1(left[0], k)\n",
    "    c_top_k = k_largest_index_argpartition_v1(center[0], k)\n",
    "    r_top_k = k_largest_index_argpartition_v1(right[0], k)\n",
    "    for i in range(k):\n",
    "        l_mask[l_top_k[i][0]][l_top_k[i][1]] = left[0][l_top_k[i][0]][l_top_k[i][1]]\n",
    "        c_mask[c_top_k[i][0]][c_top_k[i][1]] = i\n",
    "        r_mask[r_top_k[i][0]][r_top_k[i][1]] = i\n",
    "    \n",
    "    fig,axn = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(24,10))\n",
    "    sns.heatmap(left[0], ax=axn[0], cbar=False, vmin=-1.5, vmax=1.5, cmap=\"YlGnBu\", annot=True)\n",
    "    sns.heatmap(right[0], ax=axn[2], cbar=False, vmin=-1.5, vmax=1.5, cmap=\"YlGnBu\", annot=True)\n",
    "    fig.suptitle(\"Row = {}\".format(tdx))\n",
    "    fig.savefig('./plots/plot_' + str(tdx) + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mindfulness_summary_rough.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalids = df[df[\"Performance\"]<0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalids = invalids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = [str(i[0])+'_'+i[1] for i in invalids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
