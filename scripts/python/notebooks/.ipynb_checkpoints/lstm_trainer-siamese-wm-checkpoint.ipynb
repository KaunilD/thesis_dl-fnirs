{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as torch_data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import shuffle, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMValDataLoader(torch_data.Dataset):\n",
    "    def __init__(self, data_list ):\n",
    "        self.data_list = data_list\n",
    "        self.image_list, self.label_list, self.class_list = [], [], []\n",
    "        self.read_lists()\n",
    "\n",
    "    def read_lists(self):\n",
    "        \n",
    "        for idx, item in enumerate(self.data_list):\n",
    "            print(\"Reading item # {}\".format(idx), end=\"\\r\")\n",
    "            datum = item\n",
    "            im1 = datum[\"t1\"][0]\n",
    "            im2 = datum[\"t2\"][0]\n",
    "            im3 = datum[\"t3\"][0]\n",
    "            im4 = datum[\"t4\"][0]\n",
    "            #image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2], image.shape[3] ))\n",
    "            self.image_list.append((im1, im2, im3, im4))\n",
    "        print()\n",
    "    def __getitem__(self, index):\n",
    "        return self.image_list[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "class LSTMTrainDataLoader(torch_data.Dataset):\n",
    "    def __init__(self, master_data_list, count=1000 ):\n",
    "        self.master_data_list = master_data_list\n",
    "        self.count = count\n",
    "        #self.data_list = sample(self.master_data_list[0], self.count) + sample(self.master_data_list[1], self.count)\n",
    "        self.data_list = self.master_data_list[0] + self.master_data_list[1]\n",
    "        shuffle(self.data_list)\n",
    "        self.image_list, self.label_list = [], []\n",
    "        self.read_lists()\n",
    "        \n",
    "\n",
    "    def read_lists(self):\n",
    "        switch = True\n",
    "\n",
    "        for idx, item in enumerate(self.data_list):\n",
    "            print(\"Reading item # {}\".format(idx), end=\"\\r\")\n",
    "            datum = np.load(item)\n",
    "            im1 = datum[0][0]\n",
    "            im2 = datum[1][0]\n",
    "            \n",
    "            #image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2], image.shape[3] ))\n",
    "            label = datum[2]\n",
    "            \n",
    "            self.image_list.append((im1, im2))\n",
    "            self.label_list.append(label)\n",
    "            \n",
    "            del im1\n",
    "            del im2\n",
    "        print()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return tuple([self.image_list[index], self.label_list[index]])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def shuffle_dataset(self):\n",
    "        del self.image_list\n",
    "        del self.label_list\n",
    "        \n",
    "        self.image_list, self.label_list = [], []\n",
    "        self.data_list = sample(self.master_data_list[0], self.count) + sample(self.master_data_list[1], self.count)\n",
    "        shuffle(self.data_list)\n",
    "        \n",
    "        self.read_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list_0 = glob.glob('C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness/siamese/wm/train/0/*.npy')\n",
    "train_data_list_1 = glob.glob('C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness/siamese/wm/train/1/*.npy')\n",
    "\n",
    "test_data_list_0 = glob.glob('C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness/siamese/wm/test/0/*.npy')\n",
    "test_data_list_1 = glob.glob('C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness/siamese/wm/test/1/*.npy')\n",
    "\n",
    "\n",
    "val_data_list = np.load('C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//siamese/wm/validation/data_siamese_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading item # 199\n",
      "Train dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = LSTMTrainDataLoader(\n",
    "    {0: sample(train_data_list_0, 150), 1: sample(train_data_list_1, 50)}, count=10000\n",
    ")\n",
    "print(\"Train dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading item # 59\n",
      "Test dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = LSTMTrainDataLoader(\n",
    "    {0: sample(test_data_list_0, 30), 1: sample(test_data_list_1, 30)}, count=3000\n",
    ")\n",
    "print(\"Test dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading item # 0\r",
      "Reading item # 1\r",
      "Reading item # 2\r",
      "Reading item # 3\r",
      "Reading item # 4\r",
      "Reading item # 5\r",
      "Reading item # 6\r",
      "Reading item # 7\r",
      "Reading item # 8\r",
      "Reading item # 9\r",
      "Reading item # 10\r",
      "Reading item # 11\r",
      "Reading item # 12\r",
      "Reading item # 13\r",
      "Reading item # 14\r",
      "Reading item # 15\r",
      "Reading item # 16\r",
      "Reading item # 17\r",
      "Reading item # 18\r",
      "Reading item # 19\r",
      "Reading item # 20\r",
      "Reading item # 21\r",
      "Reading item # 22\r",
      "Reading item # 23\r",
      "Reading item # 24\r",
      "Reading item # 25\r",
      "Reading item # 26\r",
      "Reading item # 27\r",
      "Reading item # 28\r",
      "Reading item # 29\r",
      "Reading item # 30\r",
      "Reading item # 31\r",
      "Reading item # 32\r",
      "Reading item # 33\r",
      "Reading item # 34\r",
      "Reading item # 35\r",
      "Reading item # 36\r",
      "Reading item # 37\r",
      "Reading item # 38\r",
      "Reading item # 39\r",
      "Reading item # 40\r",
      "Reading item # 41\r",
      "Reading item # 42\r",
      "Reading item # 43\r",
      "Reading item # 44\r",
      "Reading item # 45\r",
      "Reading item # 46\r",
      "Reading item # 47\r",
      "Reading item # 48\r",
      "Reading item # 49\r",
      "Reading item # 50\r",
      "Reading item # 51\r",
      "Reading item # 52\r",
      "Reading item # 53\r",
      "Reading item # 54\r",
      "Reading item # 55\r",
      "Reading item # 56\r",
      "Reading item # 57\r",
      "Reading item # 58\r",
      "Reading item # 59\r",
      "Reading item # 60\r",
      "Reading item # 61\r",
      "Reading item # 62\r",
      "Reading item # 63\r",
      "Reading item # 64\r",
      "Reading item # 65\r",
      "Reading item # 66\r",
      "Reading item # 67\r",
      "Reading item # 68\r",
      "Reading item # 69\r",
      "Reading item # 70\r",
      "Reading item # 71\r",
      "Reading item # 72\r",
      "Reading item # 73\r",
      "Reading item # 74\r",
      "Reading item # 75\r",
      "Reading item # 76\r",
      "Reading item # 77\r",
      "Reading item # 78\r",
      "Reading item # 79\r",
      "Reading item # 80\r",
      "Reading item # 81\r",
      "Reading item # 82\r",
      "Reading item # 83\r",
      "Reading item # 84\r",
      "Reading item # 85\r",
      "Reading item # 86\r",
      "Reading item # 87\r",
      "Reading item # 88\r",
      "Reading item # 89\r",
      "Reading item # 90\r",
      "Reading item # 91\r",
      "Reading item # 92\r",
      "Reading item # 93\r",
      "Reading item # 94\r",
      "Reading item # 95\r",
      "Reading item # 96\r",
      "Reading item # 97\r",
      "Reading item # 98\r",
      "Reading item # 99\r",
      "Reading item # 100\r",
      "Reading item # 101\r",
      "Reading item # 102\r",
      "Reading item # 103\r",
      "Reading item # 104\r",
      "Reading item # 105\r",
      "Reading item # 106\r",
      "Reading item # 107\r",
      "Reading item # 108\r",
      "Reading item # 109\r",
      "Reading item # 110\r",
      "Reading item # 111\r",
      "Reading item # 112\r",
      "Reading item # 113\r",
      "Reading item # 114\r",
      "Reading item # 115\r",
      "Reading item # 116\r",
      "Reading item # 117\r",
      "Reading item # 118\r",
      "Reading item # 119\r",
      "Reading item # 120\r",
      "Reading item # 121\r",
      "Reading item # 122\r",
      "Reading item # 123\r",
      "Reading item # 124\r",
      "Reading item # 125\r",
      "Reading item # 126\r",
      "Reading item # 127\r",
      "Reading item # 128\r",
      "Reading item # 129\r",
      "Reading item # 130\r",
      "Reading item # 131\r",
      "Reading item # 132\r",
      "Reading item # 133\r",
      "Reading item # 134\r",
      "Reading item # 135\r",
      "Reading item # 136\r",
      "Reading item # 137\r",
      "Reading item # 138\r",
      "Reading item # 139\r",
      "Reading item # 140\r",
      "Reading item # 141\r",
      "Reading item # 142\r",
      "Reading item # 143\r",
      "Reading item # 144\r",
      "Reading item # 145\r",
      "Reading item # 146\r",
      "Reading item # 147\r",
      "Reading item # 148\r",
      "Reading item # 149\r",
      "Reading item # 150\r",
      "Reading item # 151\r",
      "Reading item # 152\r",
      "Reading item # 153\r",
      "Reading item # 154\r",
      "Reading item # 155\r",
      "Reading item # 156\r",
      "Reading item # 157\r",
      "Reading item # 158\r",
      "Reading item # 159\r",
      "Reading item # 160\r",
      "Reading item # 161\r",
      "Reading item # 162\r",
      "Reading item # 163\r",
      "Reading item # 164\r",
      "Reading item # 165\r",
      "Reading item # 166\r",
      "Reading item # 167\r",
      "Reading item # 168\r",
      "Reading item # 169\r",
      "Reading item # 170\r",
      "Reading item # 171\r",
      "Reading item # 172\r",
      "Reading item # 173\r",
      "Reading item # 174\r",
      "Reading item # 175\r",
      "Reading item # 176\r",
      "Reading item # 177\r",
      "Reading item # 178\r",
      "Reading item # 179\r",
      "Reading item # 180\r",
      "Reading item # 181\r",
      "Reading item # 182\r",
      "Reading item # 183\r",
      "Reading item # 184\r",
      "Reading item # 185\r",
      "Reading item # 186\r",
      "Reading item # 187\r",
      "Reading item # 188\r",
      "Reading item # 189\r",
      "Reading item # 190\r",
      "Reading item # 191\r",
      "Reading item # 192\r",
      "Reading item # 193\r",
      "Reading item # 194\r",
      "Reading item # 195\r",
      "Reading item # 196\r",
      "Reading item # 197\r",
      "Reading item # 198\r",
      "Reading item # 199\r",
      "Reading item # 200\r",
      "Reading item # 201\r",
      "Reading item # 202\r",
      "Reading item # 203\r",
      "Reading item # 204\r",
      "Reading item # 205\r",
      "Reading item # 206\r",
      "Reading item # 207\r",
      "Reading item # 208\r",
      "Reading item # 209\r",
      "Reading item # 210\r",
      "Reading item # 211\r",
      "Reading item # 212\r",
      "Reading item # 213\r",
      "Reading item # 214\r",
      "Reading item # 215\r\n",
      "Validation dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = LSTMValDataLoader(\n",
    "    val_data_list\n",
    ")\n",
    "print(\"Validation dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM2D(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size, input_channel, hidden_channel,\n",
    "            kernel_size, stride=1, padding=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializations\n",
    "        :param input_size: (int, int): height, width tuple of the input\n",
    "        :param input_channel: int: number of channels of the input\n",
    "        :param hidden_channel: int: number of channels of the hidden state\n",
    "        :param kernel_size: int: size of the filter\n",
    "        :param stride: int: stride\n",
    "        :param padding: int: width of the 0 padding\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTM2D, self).__init__()\n",
    "        self.n_h, self.n_w = input_size\n",
    "        self.n_c = input_channel\n",
    "        self.hidden_channel = hidden_channel\n",
    "\n",
    "        self.conv_xi = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_xf = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_xo = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_xg = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hi = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hf = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_ho = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hg = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hi = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden_states):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "        reference: https://arxiv.org/pdf/1506.04214.pdf (3.1)\n",
    "        :param x: input tensor of shape (n_batch, n_c, n_h, n_w)\n",
    "        :param hidden_states: (tensor, tensor) for hidden and cell states.\n",
    "                              Each of shape (n_batch, n_hc, n_hh, n_hw)\n",
    "        :return: (hidden_state, cell_state)\n",
    "        \"\"\"\n",
    "\n",
    "        hidden_state, cell_state = hidden_states\n",
    "\n",
    "        xi = self.conv_xi(x)\n",
    "        hi = self.conv_hi(hidden_state)\n",
    "        xf = self.conv_xf(x)\n",
    "        hf = self.conv_hf(hidden_state)\n",
    "        xo = self.conv_xo(x)\n",
    "        ho = self.conv_ho(hidden_state)\n",
    "        xg = self.conv_xg(x)\n",
    "        hg = self.conv_hg(hidden_state)\n",
    "\n",
    "        i = torch.sigmoid(xi + hi)\n",
    "        f = torch.sigmoid(xf + hf)\n",
    "        o = torch.sigmoid(xo + ho)\n",
    "        g = torch.tanh(xg + hg)\n",
    "        #print(f.size(), cell_state.size(), i.size(), g.size())\n",
    "        cell_state = f * cell_state + i * g\n",
    "\n",
    "        hidden_state = o * torch.tanh(cell_state)\n",
    "\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (\n",
    "                torch.zeros(batch_size, self.hidden_channel, self.n_h, self.n_w).cuda(),\n",
    "                torch.zeros(batch_size, self.hidden_channel, self.n_h, self.n_w).cuda()\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset_loader, epoch, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    for i , (data, target) in enumerate(dataset_loader):\n",
    "\n",
    "        print('Train: [Batch: {} ({:.0f}%)]'\n",
    "              .format(\n",
    "                  i + 1,\n",
    "                  (i + 1)*100/len(dataset_loader)\n",
    "              ), end='\\r')\n",
    "        \n",
    "        labels = target.long()\n",
    "        input1, input2 = data[0].float(), data[1].float() \n",
    "        input1, input2, labels = input1.to(device), input2.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # out1, out2\n",
    "        outputs = model(input1, input2)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "\n",
    "    return running_loss/total\n",
    "\n",
    "\n",
    "def test(model, dataset_loader, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i , (data, target) in enumerate(dataset_loader):\n",
    "\n",
    "        print('Test: [Batch: {} ({:.0f}%)]'\n",
    "              .format(\n",
    "                  i + 1,\n",
    "                  (i + 1)*100/len(dataset_loader)\n",
    "              ), end='\\r')\n",
    "        \n",
    "        labels = target.long()\n",
    "        input1, input2 = data[0].float(), data[1].float() \n",
    "        input1, input2, labels = input1.to(device), input2.to(device), labels.to(device)\n",
    "        \n",
    "        # out1, out2\n",
    "        outputs = model(input1, input2)\n",
    "        \n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        correct += (pred == labels).sum()\n",
    "        \n",
    "        \"\"\"\n",
    "        print(outputs)\n",
    "        print(pred)\n",
    "        print(labels)\n",
    "        print(correct)\n",
    "        \"\"\"\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss/total, 100*correct/total\n",
    "\n",
    "\n",
    "def validate(model, dataset_loader, device, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataset_loader):\n",
    "            print('Validation: [Batch: {} ({:.0f}%)]'\n",
    "                  .format(\n",
    "                      i + 1,\n",
    "                      (i + 1)*100/len(dataset_loader)\n",
    "                  ), end='\\r')\n",
    "\n",
    "            \n",
    "            input1, input2, input3, input4 = data[0].float(), data[1].float(), data[2].float(), data[3].float()\n",
    "            input1, input2, input3, input4 = input1.to(device), input2.to(device), input3.to(device), input4.to(device)\n",
    "\n",
    "            output1 = model(input1, input2)\n",
    "            output2 = model(input1, input3)\n",
    "            output3 = model(input1, input4)\n",
    "            \n",
    "            greater_mask = output3[:, 0].gt(output1[:, 0]).cpu().numpy() & output3[:, 0].gt(output2[:, 0]).cpu().numpy()\n",
    "                \n",
    "            \"\"\"\n",
    "            valid_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \"\"\"\n",
    "            total += input1.size(0)\n",
    "            correct += np.sum(greater_mask)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class ConvLSTMNet(nn.Module):\n",
    "    def __init__( self, num_classes = 2):\n",
    "        super(ConvLSTMNet, self).__init__()\n",
    "        \n",
    "        #self.conv3d1 = nn.Conv3d(in_channels=2, out_channels=15, kernel_size=(2, 2, 2))\n",
    "        #self.bn1 = nn.BatchNorm3d(15)\n",
    "        self.pool1 = nn.AvgPool3d((50, 1, 1))\n",
    "        \n",
    "        #self.conv3d2 = nn.Conv3d(in_channels=15, out_channels=30, kernel_size=(5, 1, 1), stride=(5, 2, 2))\n",
    "        #self.bn2 = nn.BatchNorm3d(30)\n",
    "        #self.pool2 = nn.AvgPool3d((1, 1, 1))\n",
    "        \n",
    "        \n",
    "        self.convLSTM2d1 = ConvLSTM2D((5, 11), 2, 8, 1)\n",
    "        self.convLSTM2d2 = ConvLSTM2D((5, 11), 8, 64, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(3520, 1760)\n",
    "        self.fc2 = nn.Linear(1760, 880)\n",
    "        self.fc3 = nn.Linear(880, 440)\n",
    "        self.fc4 = nn.Linear(440, 220)\n",
    "        self.fc5 = nn.Linear(220, 110)\n",
    "        self.fc6 = nn.Linear(110, 55)\n",
    "        self.fc7 = nn.Linear(55, 8)\n",
    "        self.fc8 = nn.Linear(8, 2)\n",
    "        \n",
    "        \n",
    "\n",
    "    def sub_forward(self, x, hidden_states= None):\n",
    "\n",
    "        b_idx, ts, n_ch, w, h = x.size()\n",
    "        if hidden_states:\n",
    "            self.h1, self.c1 = hidden_states\n",
    "        else:\n",
    "            self.h1, self.c1 = self.convLSTM2d1.init_hidden(batch_size=b_idx)\n",
    "            self.h2, self.c2 = self.convLSTM2d2.init_hidden(batch_size=b_idx)\n",
    "        #out = x\n",
    "        # N, C, D, H, W = 1, 1, 160, 5, 22\n",
    "        out = x.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        #out = self.conv3d1(out)\n",
    "        out = self.pool1(out)\n",
    "        #out = self.bn1(out)\n",
    "        \"\"\"\n",
    "        out = self.conv3d2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.bn2(out)\n",
    "        \"\"\"\n",
    "        #print(out.size())\n",
    "        # N, D, C, H, W = 1, 1, 160, 5, 22\n",
    "        out = out.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        for t in range(0, out.size(1)):\n",
    "            \n",
    "            self.h1, self.c1 = self.convLSTM2d1(\n",
    "                out[:, t, :, :, :], (self.h1, self.c1)\n",
    "            )\n",
    "\n",
    "            self.h2, self.c2 = self.convLSTM2d2(\n",
    "                self.h1, (self.h2, self.c2)\n",
    "            )\n",
    "\n",
    "        out = self.h2.view(self.h2.size(0), -1)\n",
    "\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.fc8(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self, x1, x2, hidden_states = None):\n",
    "        out1 = self.sub_forward(x1)\n",
    "        out2 = self.sub_forward(x2)\n",
    "        \n",
    "        \n",
    "        o_sum = out1 + out2\n",
    "        o_prod = out1 * out2\n",
    "        o_abs = torch.abs(out1 - out2)\n",
    "        o_sq = torch.pow(o_abs, 2)\n",
    "        \n",
    "        out = torch.cat((o_sum, o_prod, o_abs, o_sq))\n",
    "        out = out.view(-1, 8)\n",
    "        out = self.fc8(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available()   = True\n",
      "torch.cuda.device_count()   = 1\n",
      "torch.cuda.device('cuda')   = <torch.cuda.device object at 0x000001E6FF22C6A0>\n",
      "torch.cuda.current_device() = 0\n",
      "Epoch\tTrain Loss\tTest Loss\tTest Accuracy\tValidation Accuracy\n",
      "Train: [Batch: 100 (100%)]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(\"torch.cuda.is_available()   =\", torch.cuda.is_available())\n",
    "    print(\"torch.cuda.device_count()   =\", torch.cuda.device_count())\n",
    "    print(\"torch.cuda.device('cuda')   =\", torch.cuda.device('cuda'))\n",
    "    print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ConvLSTMNet()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, verbose=True)\n",
    "    epochs = 30\n",
    "    curr_epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_loss_history, test_loss_history = [], []\n",
    "    test_acc_history, val_acc_history = [], []\n",
    "    \n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    best_model = -1\n",
    "\n",
    "    MODEL_PATH_PREFIX = 'model-convlstm-epoch'\n",
    "    MODEL_PATH_EXT = 'pth'\n",
    "\n",
    "    train_loader = torch_data.DataLoader(\n",
    "        train_dataloader,\n",
    "        batch_size=256, shuffle=True, num_workers=0\n",
    "    )\n",
    "    \n",
    "    test_loader = torch_data.DataLoader(\n",
    "        test_dataloader,\n",
    "        batch_size=128, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = torch_data.DataLoader(\n",
    "        val_dataloader,\n",
    "        batch_size=16, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    is_best = True\n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    \n",
    "    print(\"Epoch\\tTrain Loss\\tTest Loss\\tTest Accuracy\\tValidation Accuracy\")\n",
    "    while curr_epoch <= epochs:\n",
    "        \n",
    "        train_running_loss = train(\n",
    "            model, train_loader,\n",
    "            curr_epoch, device, optimizer, criterion\n",
    "        )\n",
    "        break\n",
    "    \n",
    "        test_running_loss, test_accuracy = test(\n",
    "            model, test_loader,\n",
    "            device, criterion\n",
    "        )\n",
    "        \n",
    "        val_accuracy = validate(\n",
    "            model, val_loader,\n",
    "            device, criterion\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_accuracy)\n",
    "        # record all the models that we have had so far.\n",
    "        train_loss_history.append(\n",
    "            train_running_loss\n",
    "        )\n",
    "\n",
    "        test_acc_history.append(\n",
    "            test_accuracy\n",
    "        )\n",
    "        \n",
    "        test_loss_history.append(\n",
    "            test_running_loss\n",
    "        )\n",
    "        \n",
    "        val_acc_history.append(\n",
    "            val_accuracy\n",
    "        )\n",
    "        # write model to disk.\n",
    "\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        \n",
    "            \n",
    "        'models/model-siamese-epoch'\n",
    "        torch.save(\n",
    "            state,\n",
    "            MODEL_PATH_PREFIX + '-{}.'.format(curr_epoch) + MODEL_PATH_EXT\n",
    "        )\n",
    "\n",
    "        print('{}\\t{:.5f}\\t\\t{:.5f}\\t\\t{:.5f}\\t\\t{:.5f}\\t\\t'.format(\n",
    "            curr_epoch,\n",
    "            train_running_loss,\n",
    "            test_running_loss,\n",
    "            test_accuracy,\n",
    "            val_accuracy\n",
    "        ))\n",
    "        curr_epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_path = 'model-convlstm-epoch-18.pth'\n",
    "model = ConvLSTMNet()\n",
    "model.load_state_dict(torch.load(model_path)[\"model\"])\n",
    "model.eval()\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "true = []\n",
    "pred = []\n",
    "for data, lab in test_dataloader:\n",
    "    x1 = data[0]\n",
    "    x2 = data[1]\n",
    "    x1 = torch.from_numpy(np.array([x1])).float()\n",
    "    x2 = torch.from_numpy(np.array([x2])).float()\n",
    "    out = model(x1, x2)\n",
    "\n",
    "    #0 = matching, 1 = different\n",
    "    match = torch.max(out.data,  1)[1].numpy()[0]\n",
    "    true.append(lab)\n",
    "    pred.append(match)\n",
    "    #print(lab, \"match\" if match == 0 else \"different\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "cnf = confusion_matrix(y_pred=pred, y_true=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=[0, 1], yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(true, pred,  classes=[\"0\", \"1\"], normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 && 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
