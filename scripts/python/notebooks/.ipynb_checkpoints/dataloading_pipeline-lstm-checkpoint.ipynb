{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import tables\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Trevor Jordan Grant.\n",
    "default4: spatial/verbal\n",
    "\"\"\"\n",
    "# Dictionary is labeled such that task labels in conditions files will have\n",
    "# more than one multilabeling schema.\n",
    "\n",
    "# To add more multilabeling schema - include them in the task Dictionary.\n",
    "# (The sub-dictionary where the task label is the key.)\n",
    "\n",
    "# default4 = [\"VerbalWM\", \"SpatialWM\", \"VisualPerceptual\", \"AuditoryPerceptual\"]\n",
    "# every label in default 4 has discrete values of 'off', 'low', 'high'\n",
    "\n",
    "# default3 = [\"WM\", \"VisualPerceptual\", \"AuditoryPerceptual\"]\n",
    "# every label in default 3 has discrete values of 'off', 'low', 'high'\n",
    "\n",
    "cog_load_label_dict = {\n",
    "# Mindfulness task labels.\n",
    "                       \"nb\": {\n",
    "                              \"default4\": [\"high\", \"off\", \"low\", \"off\"],\n",
    "                              \"default3\": [\"high\", \"low\", \"off\"],\n",
    "                             },\n",
    "                       \"anb\": {\n",
    "                               \"default4\": [\"high\", \"off\", \"off\", \"low\"],\n",
    "                               \"default3\": [\"high\", \"off\", \"low\"],\n",
    "                              },\n",
    "                       \"ewm\": {\n",
    "                               \"default4\": [\"low\", \"off\", \"high\", \"off\"],\n",
    "                               \"default3\": [\"low\", \"high\", \"off\"]\n",
    "                              },\n",
    "                        \"cr\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"off\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"off\", \"off\"],\n",
    "                              },\n",
    "                        \"rt\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                        \"es\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                       \"gng\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "            \"adaptive_words\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                   \"go_nogo\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                     \"nback\": {\n",
    "                               \"default4\": [\"high\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"high\", \"low\", \"off\"],\n",
    "                              },\n",
    "                    \"posner\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "                 \"simple_rt\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"low\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"low\", \"off\"],\n",
    "                              },\n",
    "             \"visual_search\": {\n",
    "                               \"default4\": [\"off\", \"off\", \"high\", \"off\"],\n",
    "                               \"default3\": [\"off\", \"high\", \"off\"],\n",
    "                              },\n",
    "                      }\n",
    "\n",
    "\n",
    "def strings_to_vectors(string_labels, as_list=False):\n",
    "    \"\"\"Maps strings in dict to interger values.\n",
    "    Args:\n",
    "        string_labels(list): The string label value of load.\n",
    "        as_list(bool): False, if True, return list instead of np.array()\n",
    "    Returns:\n",
    "        labels as np.array()\n",
    "    \"\"\"\n",
    "\n",
    "    maps = {\n",
    "            \"off\": 0,\n",
    "            \"low\": 1,\n",
    "            \"high\": 2,\n",
    "           }\n",
    "\n",
    "    if as_list:\n",
    "        return [maps[label] for label in string_labels]\n",
    "    return np.array([maps[label] for label in string_labels])\n",
    "\n",
    "\n",
    "def return_label(task, label_type=\"default3\", as_strings=False):\n",
    "    \"\"\"Returns a label from the cog_load_label_dict.\n",
    "    Args:\n",
    "        task(str): The task label from the coditions file.\n",
    "        label_type(string): The label schema used for the model.\n",
    "        as_strings(bool): False, if True, return string (in list) values instead.\n",
    "    Returns:\n",
    "        labels(np.array): Under defaults labels will be returned as interger\n",
    "        values in a np.array().\n",
    "    \"\"\"\n",
    "    if as_strings:\n",
    "        return cog_load_label_dict[task][label_type]\n",
    "    return strings_to_vectors(cog_load_label_dict[task][label_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_52_5x11_mat = {\n",
    "           1:[0,1],  2:[0,2],  3:[0,3],  4:[0,4],  5:[0,5],  6:[0,6],  7:[0,7],  8:[0,8],  9:[0,9], 10:[0,10], \n",
    "11:[1,0], 12:[1,1], 13:[1,2], 14:[1,3], 15:[1,4], 16:[1,5], 17:[1,6], 18:[1,7], 19:[1,8], 20:[1,9], 21:[1,10], \n",
    "          22:[2,1], 23:[2,2], 24:[2,3], 25:[2,4], 26:[2,5], 27:[2,6], 28:[2,7], 29:[2,8], 30:[2,9], 31:[2,10], \n",
    "32:[3,0], 33:[3,1], 34:[3,2], 35:[3,3], 36:[3,4], 37:[3,5], 38:[3,6], 39:[3,7], 40:[3,8], 41:[3,9], 42:[3,10], \n",
    "          43:[4,1], 44:[4,2], 45:[4,3], 46:[4,4], 47:[4,5], 48:[4,6], 49:[4,7], 50:[4,8], 51:[4,9], 52:[4,10]\n",
    "}\n",
    "\n",
    "def get_52_5x11_mat(data):\n",
    "    # returns a matrix of size 5x11.\n",
    "    mat = np.zeros((5, 11))\n",
    "    for idx, i in enumerate((data)):\n",
    "        loc = channel_52_5x11_mat[idx+1]\n",
    "        mat[loc[0], loc[1]] = i\n",
    "    return mat\n",
    "\n",
    "channel_52_5x22_mat = {\n",
    "           1:[0,1],  2:[0,3],  3:[0,5],  4:[0,7],  5:[0,9],  6:[0,11],  7:[0,13],  8:[0,15],  9:[0,17], 10:[0,19], \n",
    "11:[1,0], 12:[1,2], 13:[1,4], 14:[1,6], 15:[1,8], 16:[1,10], 17:[1,12], 18:[1,14], 19:[1,16], 20:[1,18], 21:[1,20], \n",
    "          22:[2,1], 23:[2,3], 24:[2,5], 25:[2,7], 26:[2,9], 27:[2,11], 28:[2,13], 29:[2,15], 30:[2,17], 31:[2,19], \n",
    "32:[3,0], 33:[3,2], 34:[3,4], 35:[3,6], 36:[3,8], 37:[3,10], 38:[3,12], 39:[3,14], 40:[3,16], 41:[3,18], 42:[3,20], \n",
    "          43:[4,1], 44:[4,3], 45:[4,5], 46:[4,7], 47:[4,9], 48:[4,11], 49:[4,13], 50:[4,15], 51:[4,17], 52:[4,19]\n",
    "}\n",
    "\n",
    "def get_52_5x22_mat(data):\n",
    "    # returns a matrix of size 5x11.\n",
    "    mat = np.zeros((5, 22))\n",
    "    for idx, i in enumerate((data)):\n",
    "        loc = channel_52_5x22_mat[idx+1]\n",
    "        mat[loc[0], loc[1]] = i\n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_tasks(tasks, min_dur):\n",
    "    collapsed_tasks = []\n",
    "    collapsed_tasks.append(\n",
    "        tasks[0]\n",
    "    )\n",
    "    for i in range(1, len(tasks)):\n",
    "        t1 = collapsed_tasks[-1]\n",
    "        t2 = tasks[i]\n",
    "        if t1[\"class\"] == t2[\"class\"] and (t2[\"duration\"] < min_dur and t1[\"duration\"] < min_dur):\n",
    "            \n",
    "            t1[\"data\"] = np.concatenate((t1[\"data\"], t2[\"data\"]), axis=0)\n",
    "            t1[\"duration\"]+=t2[\"duration\"]\n",
    "            t1[\"end\"]=t1[\"onset\"]+t2[\"duration\"]\n",
    "            # merge and append\n",
    "        else:\n",
    "            # just append\n",
    "            collapsed_tasks.append(t2)\n",
    "    return collapsed_tasks\n",
    "            \n",
    "def read_tasks(condition, data):\n",
    "    # conditions, data = csv, mat files\n",
    "    # tuple containing (class, onset, duration, offset, oxy_data, dxy_data)\n",
    "    tasks = []\n",
    "    # read conditions, data\n",
    "    c_data = pd.read_csv(condition)\n",
    "    m_data = loadmat(data)\n",
    "    # get oxy, dxy data\n",
    "    oxyDaya = m_data['nirs_data'][0][0][0]\n",
    "    dxyData = m_data['nirs_data'][0][0][1]\n",
    "    # iterate through all the tasks here now.\n",
    "    for idx, key in enumerate(list(c_data.keys())):\n",
    "        start = 0\n",
    "        end = 0\n",
    "        class_ = None\n",
    "        if 'Task' in key or 'all_benchmarks_fNIRS' in key:\n",
    "            # get start and end index of the task\n",
    "            if 'Task' in key:\n",
    "                start = int(c_data[key][0])\n",
    "                duration = int(c_data[key][1])\n",
    "                class_ = c_data[key][2]\n",
    "            else:\n",
    "                start = int(c_data[key][2])\n",
    "                duration = int(c_data[key][3]) \n",
    "                class_ = c_data[key][4]\n",
    "            end = start + duration\n",
    "            \n",
    "            # visualize heatmap: \n",
    "            # sns.heatmap(get_52_mat(oxyDaya[0]))\n",
    "\n",
    "            oxy_series = oxyDaya[start:end, :]\n",
    "            dxy_series = dxyData[start:end, :]\n",
    "\n",
    "            # a 100x5x22 list\n",
    "            oxy_dxy_series_mat = np.zeros((duration,2, 5, 11))\n",
    "\n",
    "            for ts, (oxy_slice, dxy_slice) in enumerate(zip(oxy_series, dxy_series)):\n",
    "                oxy_slice = get_52_5x11_mat(oxy_slice)\n",
    "                dxy_slice = get_52_5x11_mat(dxy_slice)\n",
    "                \n",
    "                min_oxy = oxy_slice.min()\n",
    "                max_oxy = oxy_slice.max()\n",
    "                \n",
    "                min_dxy = dxy_slice.min()\n",
    "                max_dxy = dxy_slice.max()\n",
    "                \n",
    "                oxy_slice -= min_oxy\n",
    "                oxy_slice /= (max_oxy-min_oxy)\n",
    "                \n",
    "                dxy_slice -= min_dxy\n",
    "                dxy_slice /= (max_dxy-min_dxy)\n",
    "                \n",
    "                return\n",
    "                #oxy_dxy_series_mat[ts] = np.hstack([oxy_slice, dxy_slice])\n",
    "                oxy_dxy_series_mat[ts] = np.array([oxy_slice, dxy_slice])\n",
    "            tasks.append(\n",
    "                {\n",
    "                    \"class\": class_,\n",
    "                    \"onset\": start,\n",
    "                    \"end\": end,\n",
    "                    \"duration\": duration,\n",
    "                    \"data\" : oxy_dxy_series_mat\n",
    "                }\n",
    "            )\n",
    "    return tasks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = glob.glob('../../../data/multilabel/mats/*fNIRS_conditions_*.csv')\n",
    "data = glob.glob('../../../data/multilabel/mats/*Probe1*.csv.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.20481133 -3.16809392 -5.06882762  0.18250223  0.70615012\n",
      "   0.98646095 -0.19107392 -0.67957757 -0.83815117 -2.72416271]\n",
      " [ 0.78657591  2.24510839 -0.75031103 -0.21772256  0.07417767  0.21952608\n",
      "   0.73168103 -0.62922919  0.42671504  0.12687672 -5.40134291]\n",
      " [ 0.          0.9855983   1.12488797  0.13940219 -0.13266753 -0.09313401\n",
      "   0.0190898  -0.05091243  0.1671092   0.65432659  0.53784984]\n",
      " [-4.71023814  1.16601537  0.70686516 -0.03954834  0.20884067  0.04887098\n",
      "   0.04083194  0.57871167  0.44867039  0.94927314  0.77743901]\n",
      " [ 0.         -7.95524432  0.72220745  0.33023701  0.32665262  0.34942904\n",
      "   0.05405582 -0.05208975  0.18028628  0.33712702  0.86230347]] -7.955244317787878 2.245108385129793\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-000c2cdfe3b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mparticipant_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_tasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mtask_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtask_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparticipant_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "task_data = []\n",
    "time_series_length = 10\n",
    "\"\"\"\n",
    "default3 labels\n",
    "[ \n",
    "    wm, \n",
    "    v, \n",
    "    a\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "for idx, (cond, dat) in enumerate(zip(conditions, data)):\n",
    "    participant_id = os.path.basename(dat)[0:4]\n",
    "    tasks = read_tasks(cond, dat)\n",
    "    for t in tasks:\n",
    "        task_data.append(t)\n",
    "        task_data[-1][\"id\"] = participant_id\n",
    "        task_data[-1][\"wl_label\"] = return_label(task_data[-1][\"class\"])\n",
    "    task_data = collapse_tasks(task_data, min_dur=time_series_length)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_bin = {\"wm\":{0:0, 1:0, 2:0}, \"vl\":{0:0, 1:0, 2:0}, \"al\":{0:0, 1:0, 2:0}}\n",
    "task_cond_bin = {i:{\"ts\":0, \"cnt\":0} for i in cog_load_label_dict}\n",
    "for t in task_data:\n",
    "    label = return_label(t[\"class\"])\n",
    "    \n",
    "    task_cond_bin[t[\"class\"]][\"cnt\"]+=1\n",
    "    task_cond_bin[t[\"class\"]][\"ts\"] = max(task_cond_bin[t[\"class\"]][\"ts\"], t[\"duration\"])\n",
    "    \n",
    "    labels_bin[\"wm\"][label[0]]+=1\n",
    "    labels_bin[\"vl\"][label[1]]+=1\n",
    "    labels_bin[\"al\"][label[2]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb': {'ts': 900, 'cnt': 64}, 'rt': {'ts': 650, 'cnt': 64}, 'posner': {'ts': 80, 'cnt': 728}, 'simple_rt': {'ts': 80, 'cnt': 156}, 'nback': {'ts': 80, 'cnt': 1686}, 'adaptive_words': {'ts': 80, 'cnt': 436}, 'visual_search': {'ts': 80, 'cnt': 672}, 'ewm': {'ts': 3000, 'cnt': 64}, 'go_nogo': {'ts': 80, 'cnt': 727}, 'cr': {'ts': 250, 'cnt': 384}, 'es': {'ts': 700, 'cnt': 64}, 'gng': {'ts': 850, 'cnt': 64}, 'anb': {'ts': 1000, 'cnt': 64}} {'wm': {0: 3295, 1: 64, 2: 1814}, 'vl': {0: 448, 1: 3989, 2: 736}, 'al': {0: 5109, 1: 64, 2: 0}}\n"
     ]
    }
   ],
   "source": [
    "print(task_cond_bin, labels_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    write all data to disk as is\n",
    "\"\"\"\n",
    "for idx, data in enumerate(task_data):\n",
    "    np.save(\"C:\\\\Users\\\\dhruv\\\\Development\\\\git\\\\thesis_dl-fnirs\\\\data\\\\multilabel\\\\all\\\\\" + str(idx), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
