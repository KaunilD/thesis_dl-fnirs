{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "# pytorch stuff\n",
    "import torch\n",
    "\n",
    "import torch.utils.data as torch_data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMValDataLoader(torch_data.Dataset):\n",
    "    def __init__(self, data_list ):\n",
    "        self.data_list = data_list\n",
    "        self.image_list, self.label_list, self.class_list = [], [], []\n",
    "        self.read_lists()\n",
    "\n",
    "    def read_lists(self):\n",
    "\n",
    "        for idx, item in enumerate(self.data_list):\n",
    "            print(\"Reading item # {}\".format(idx), end=\"\\r\")\n",
    "            datum = item\n",
    "            im1 = datum[\"t1\"][0]\n",
    "            im2 = datum[\"t2\"][0]\n",
    "            im3 = datum[\"t3\"][0]\n",
    "            #image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2], image.shape[3] ))\n",
    "            self.image_list.append((im1[0:60], im2[0:60], im3[0:60]))\n",
    "        print()\n",
    "    def __getitem__(self, index):\n",
    "        return self.image_list[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "\n",
    "class ConvLSTM2D(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size, input_channel, hidden_channel,\n",
    "            kernel_size, stride=1, padding=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializations\n",
    "        :param input_size: (int, int): height, width tuple of the input\n",
    "        :param input_channel: int: number of channels of the input\n",
    "        :param hidden_channel: int: number of channels of the hidden state\n",
    "        :param kernel_size: int: size of the filter\n",
    "        :param stride: int: stride\n",
    "        :param padding: int: width of the 0 padding\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTM2D, self).__init__()\n",
    "        self.n_h, self.n_w = input_size\n",
    "        self.n_c = input_channel\n",
    "        self.hidden_channel = hidden_channel\n",
    "\n",
    "        self.conv_xi = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_xf = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_xo = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_xg = nn.Conv2d(\n",
    "            self.n_c,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hi = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hf = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_ho = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hg = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.conv_hi = nn.Conv2d(\n",
    "            self.hidden_channel,\n",
    "            self.hidden_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden_states):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "        reference: https://arxiv.org/pdf/1506.04214.pdf (3.1)\n",
    "        :param x: input tensor of shape (n_batch, n_c, n_h, n_w)\n",
    "        :param hidden_states: (tensor, tensor) for hidden and cell states.\n",
    "                              Each of shape (n_batch, n_hc, n_hh, n_hw)\n",
    "        :return: (hidden_state, cell_state)\n",
    "        \"\"\"\n",
    "\n",
    "        hidden_state, cell_state = hidden_states\n",
    "\n",
    "        xi = self.conv_xi(x)\n",
    "        hi = self.conv_hi(hidden_state)\n",
    "        xf = self.conv_xf(x)\n",
    "        hf = self.conv_hf(hidden_state)\n",
    "        xo = self.conv_xo(x)\n",
    "        ho = self.conv_ho(hidden_state)\n",
    "        xg = self.conv_xg(x)\n",
    "        hg = self.conv_hg(hidden_state)\n",
    "\n",
    "        i = torch.sigmoid(xi + hi)\n",
    "        f = torch.sigmoid(xf + hf)\n",
    "        o = torch.sigmoid(xo + ho)\n",
    "        g = torch.tanh(xg + hg)\n",
    "        #print(f.size(), cell_state.size(), i.size(), g.size())\n",
    "        cell_state = f * cell_state + i * g\n",
    "\n",
    "        hidden_state = o * torch.tanh(cell_state)\n",
    "\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (\n",
    "                torch.zeros(batch_size, self.hidden_channel, self.n_h, self.n_w).cuda(),\n",
    "                torch.zeros(batch_size, self.hidden_channel, self.n_h, self.n_w).cuda()\n",
    "               )\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive\n",
    "\n",
    "class ConvLSTMNet(nn.Module):\n",
    "    def __init__( self, num_classes = 2):\n",
    "        super(ConvLSTMNet, self).__init__()\n",
    "\n",
    "        self.conv3d1 = nn.Conv3d(in_channels=2, out_channels=2, kernel_size=(1, 2, 2))\n",
    "        self.bn1 = nn.BatchNorm3d(15)\n",
    "        self.pool1 = nn.MaxPool3d((5, 1, 1))\n",
    "\n",
    "        self.conv3d2 = nn.Conv3d(in_channels=15, out_channels=30, kernel_size=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(30)\n",
    "        self.pool2 = nn.MaxPool3d((2, 1, 1))\n",
    "\n",
    "        self.nl1 = nn.Tanh()\n",
    "\n",
    "        self.convLSTM2d1 = ConvLSTM2D((4, 10), 2, 64, 1)\n",
    "        self.convLSTM2d2 = ConvLSTM2D((4, 10), 2, 64, 1)\n",
    "\n",
    "        self.fc2 = nn.Linear(5120, 3400)\n",
    "        self.fc3 = nn.Linear(3400, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 50)\n",
    "\n",
    "\n",
    "\n",
    "    def sub_forward(self, x, hidden_states= None):\n",
    "\n",
    "        b_idx, ts, n_ch, w, h = x.size()\n",
    "        if hidden_states:\n",
    "            self.h1, self.c1 = hidden_states\n",
    "        else:\n",
    "            self.h1, self.c1 = self.convLSTM2d1.init_hidden(batch_size=b_idx)\n",
    "            self.h2, self.c2 = self.convLSTM2d2.init_hidden(batch_size=b_idx)\n",
    "        out = x\n",
    "        # N, C, D, H, W = 1, 1, 160, 5, 22\n",
    "        out = x.permute(0, 2, 1, 3, 4)\n",
    "        out = self.conv3d1(out)\n",
    "        out = self.pool1(out)\n",
    "\n",
    "        # N, D, C, H, W = 1, 1, 160, 5, 22\n",
    "        out = out.permute(0, 2, 1, 3, 4)\n",
    "        \"\"\"\n",
    "\n",
    "        #out = self.bn1(out)\n",
    "\n",
    "\n",
    "        out = self.conv3d2(out)\n",
    "        #out = self.pool2(out)\n",
    "        #out = self.bn2(out)\n",
    "\n",
    "        out = self.nl1(out)\n",
    "\n",
    "        #print(out.size())\n",
    "        \"\"\"\n",
    "        for t in range(0, out.size(1)):\n",
    "\n",
    "            self.h1, self.c1 = self.convLSTM2d1(\n",
    "                out[:, t, :, :, :], (self.h1, self.c1)\n",
    "            )\n",
    "\n",
    "            self.h2, self.c2 = self.convLSTM2d2(\n",
    "                out[:, out.size(1)-t-1, :, :, :], (self.h2, self.c2)\n",
    "            )\n",
    "            \"\"\"\n",
    "            self.h3, self.c3 = self.convLSTM2d3(\n",
    "                self.h2, (self.h3, self.c3)\n",
    "            )\n",
    "            \"\"\"\n",
    "        out = torch.cat((self.h1, self.h2), 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x1, x2, hidden_states = None):\n",
    "        out1 = self.sub_forward(x1)\n",
    "        out2 = self.sub_forward(x2)\n",
    "\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../deep-learning/experiments/convlstm-siamese/bi-convlstm/000/model-siamese-epoch-0.pth'\n",
    "model = ConvLSTMNet()\n",
    "model.load_state_dict(torch.load(model_path)[\"model\"])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMDataLoader(torch_data.Dataset):\n",
    "    def __init__(self, data_list ):\n",
    "        self.data_list = data_list\n",
    "        self.image_list, self.label_list, self.class_list = [], [], []\n",
    "        self.read_lists()\n",
    "\n",
    "    def read_lists(self):\n",
    "        switch = True\n",
    "\n",
    "        for item in self.data_list:\n",
    "            datum = item\n",
    "\n",
    "            image = datum[\"data\"][50:250]\n",
    "            #image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2], image.shape[3] ))\n",
    "            class_ = datum[\"class\"]\n",
    "            label = datum[\"wl_label\"][0]\n",
    "            \n",
    "            self.image_list.append(image)\n",
    "            self.label_list.append(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple([self.image_list[index], self.label_list[index]])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "# In[47]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = np.load('C://Users//dhruv//Development//git//thesis_dl-fnirs//data//multilabel//all//mindfulness//data.npy')\n",
    "val_dataset = LSTMDataLoader(data_list[int(0.8*len(data_list)):])\n",
    "val_dataloader = torch_data.DataLoader(\n",
    "    val_dataset, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_dataset)):\n",
    "    for j in range(len(val_dataset)):\n",
    "        x1 = val_dataset.__getitem__(i)\n",
    "        x2 = val_dataset.__getitem__(j)\n",
    "\n",
    "        x1 = torch.from_numpy(np.array([x1[0]])).float().cpu()\n",
    "        x2 = torch.from_numpy(np.array([x2[0]])).float().cpu()\n",
    "        out = model(x1, x2)\n",
    "\n",
    "        #0 = matching, 1 = different\n",
    "        match = torch.max(out.data,  1)[1].numpy()[0]\n",
    "        print(x1[1], x2[1], \"match\" if match == 0 else \"different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Working Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "true = []\n",
    "with torch.no_grad():\n",
    "    for data, target in val_dataloader:\n",
    "        data, target = data.float(), target.long()\n",
    "        #data, target_cu = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        pred.extend(predicted.numpy())\n",
    "        true.extend(target.numpy())\n",
    "confusion_matrix(y_pred=pred, y_true=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
