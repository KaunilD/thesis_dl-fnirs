##### Experiment: 002


| hyp-params    | value                    |
| :------------ | ------------------------ |
| epochs        | 30                       |
| loss          | MultiLabelSoftMarginLoss |
| optimizer     | Adam                     |
| learning rate | 0.001                    |

##### Model: 3D CNN

``` 
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv3d-1          [-1, 5, 5, 22, 1]             205
            Conv3d-2          [-1, 2, 2, 11, 1]              12
            Conv3d-3           [-1, 1, 1, 5, 1]               3
            Linear-4                   [-1, 12]              72
           Sigmoid-5                   [-1, 12]               0
================================================================
Total params: 292
Trainable params: 292
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.02
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.02
----------------------------------------------------------------
```

##### Data

| param              | value                                                        |
| :----------------- | ------------------------------------------------------------ |
| timesteps          | 40                                                           |
| normalization      | none                                                         |
| source experiments | only session 1 from [mindfulness/benchmark_tasks/fNIRS_Data](https://github.com/lmhirshf/mindfulness/tree/master/benchmark_tasks/data/fNIRS_Data) |
| label type         | multilabel; default4                                         |
| label config       | [ wm_o, wm_l, wm_h, v_o, v_l, v_h, a_o, a_l, a_h, ewm_o, ewm_l, ewm_h ] |

##### Training

```
Epoch   Train Loss      Validation Loss Validation Acc
0       398.83376       24.39126        51.282
1       377.20693       24.42058        58.974
2       376.07661       24.42666        58.974
3       375.91635       24.42463        58.974
4       375.72677       24.42788        58.974
5       375.51731       24.42682        58.974
6       375.52606       24.42643        58.974
7       375.48138       24.42595        58.974
8       375.46186       24.42495        58.974
9       376.07420       24.42572        58.974
10      375.45695       24.42343        58.974
11      375.58358       24.42245        58.974
12      375.44913       24.42373        58.974
13      375.44272       24.42288        58.974
14      375.45629       24.42219        58.974
15      375.43686       24.42132        58.974
16      375.45311       24.42047        58.974
17      375.44551       24.42048        58.974
18      375.47065       24.42096        58.974
19      375.43883       24.41911        58.974
20      375.48745       24.42069        58.974
21      375.41271       24.41842        58.974
22      375.39729       24.41516        58.974
23      375.39995       24.41424        58.974
24      375.41808       24.41129        58.974
25      375.45017       24.41225        58.974
26      375.43638       24.41554        58.974
27      375.38976       24.41563        58.974
28      375.39279       24.41338        58.974
29      375.99000       24.41918        58.974
30      375.93079       24.41231        58.974
```

